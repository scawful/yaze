<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>yaze: LLM Integration Plan for z3ed Agent System</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript">var page_layout=1;</script>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtreedata.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/searchdata.js"></script>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
  $(document).ready(function() { init_search(); });
/* @license-end */
</script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="side-nav" class="ui-resizable side-nav-resizable"><!-- do not remove this div, it is closed by doxygen! -->
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="../../yaze.ico"/></td>
  <td id="projectalign">
   <div id="projectname">yaze<span id="projectnumber">&#160;0.3.1</span>
   </div>
   <div id="projectbrief">Link to the Past ROM Editor</div>
  </td>
 </tr>
   <tr><td colspan="2">        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect"                onmouseover="return searchBox.OnSearchSelectShow()"                onmouseout="return searchBox.OnSearchSelectHide()">&#160;</span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.svg" alt=""/></a>
          </span>
        </div>
</td></tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "../../search/",'.html');
/* @license-end */
</script>
</div><!-- top -->
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('de/da5/md_docs_2z3ed_2LLM-INTEGRATION-PLAN.html','../../'); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">LLM Integration Plan for z3ed Agent System</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md1619"></a> <b>Status</b>: Implementation Ready | Priority: High <br  />
 <b>Created</b>: October 3, 2025 <br  />
 <b>Estimated Time</b>: 12-15 hours</p>
<h1><a class="anchor" id="autotoc_md1620"></a>
Executive Summary</h1>
<p>This document outlines the practical implementation plan for integrating LLM capabilities into the z3ed agent system. The infrastructure is <b>already in place</b> with the <code>AIService</code> interface, <code>MockAIService</code> for testing, and a partially implemented <code>GeminiAIService</code>. This plan focuses on making LLM integration production-ready with both local (Ollama) and remote (Gemini, Claude) options.</p>
<p><b>Current State</b>:</p><ul>
<li>✅ <code>AIService</code> interface defined (<code><a class="el" href="../../d7/d52/ai__service_8h.html">src/cli/service/ai_service.h</a></code>)</li>
<li>✅ <code>MockAIService</code> operational (returns hardcoded test commands)</li>
<li>✅ <code>GeminiAIService</code> skeleton implemented (needs fixes + proper prompting)</li>
<li>✅ Agent workflow fully functional with proposal system</li>
<li>✅ Resource catalogue (command schemas) ready for LLM consumption</li>
<li>✅ GUI automation harness operational for verification</li>
</ul>
<p><b>What's Missing</b>:</p><ul>
<li>🔧 Ollama integration for local LLM support</li>
<li>🔧 Improved Gemini prompting with resource catalogue</li>
<li>🔧 Claude API integration as alternative remote option</li>
<li>🔧 AI service selection mechanism (env vars + CLI flags)</li>
<li>🔧 Proper prompt engineering with system instructions</li>
<li>🔧 Error handling and retry logic for API failures</li>
<li>🔧 Token usage monitoring and cost tracking</li>
</ul>
<hr  />
<h1><a class="anchor" id="autotoc_md1622"></a>
1. Implementation Priorities</h1>
<h2><a class="anchor" id="autotoc_md1623"></a>
Phase 1: Ollama Local Integration (4-6 hours) 🎯 START HERE</h2>
<p><b>Rationale</b>: Ollama provides the fastest path to a working LLM agent with no API keys, costs, or rate limits. Perfect for development and testing.</p>
<p><b>Benefits</b>:</p><ul>
<li><b>Privacy</b>: All processing happens locally</li>
<li><b>Zero Cost</b>: No API charges or token limits</li>
<li><b>Offline</b>: Works without internet connection</li>
<li><b>Fast Iteration</b>: No rate limits for testing</li>
<li><b>Model Flexibility</b>: Easily swap between codellama, llama3, qwen2.5-coder, etc.</li>
</ul>
<h3><a class="anchor" id="autotoc_md1624"></a>
1.1. Create OllamaAIService Class</h3>
<p><b>File</b>: <code><a class="el" href="../../da/d1c/ollama__ai__service_8h.html">src/cli/service/ollama_ai_service.h</a></code></p>
<div class="fragment"><div class="line"><span class="preprocessor">#ifndef YAZE_SRC_CLI_OLLAMA_AI_SERVICE_H_</span></div>
<div class="line"><span class="preprocessor">#define YAZE_SRC_CLI_OLLAMA_AI_SERVICE_H_</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &lt;string&gt;</span></div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &quot;absl/status/status.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;absl/status/statusor.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;<a class="code" href="../../d7/d52/ai__service_8h.html">cli/service/ai_service.h</a>&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">namespace </span><a class="code hl_namespace" href="../../dc/d46/namespaceyaze.html">yaze</a> {</div>
<div class="line"><span class="keyword">namespace </span>cli {</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Ollama configuration for local LLM inference</span></div>
<div class="line"><span class="keyword">struct </span>OllamaConfig {</div>
<div class="line">  std::string <a class="code hl_variable" href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#ae1bf7f1a0f72507b5e4da8794caece1f">base_url</a> = <span class="stringliteral">&quot;http://localhost:11434&quot;</span>;  <span class="comment">// Default Ollama endpoint</span></div>
<div class="line">  std::string <a class="code hl_variable" href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#afe9a8786f6353ee32bc6add2959db511">model</a> = <span class="stringliteral">&quot;qwen2.5-coder:7b&quot;</span>;           <span class="comment">// Recommended for code generation</span></div>
<div class="line">  <span class="keywordtype">float</span> <a class="code hl_variable" href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#aba1021dcaea2e27964f155388b5a6d55">temperature</a> = 0.1;                          <span class="comment">// Low temp for deterministic commands</span></div>
<div class="line">  <span class="keywordtype">int</span> <a class="code hl_variable" href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#aa219f4c0ee7685f810d8fdc2742ef19a">max_tokens</a> = 2048;                            <span class="comment">// Sufficient for command lists</span></div>
<div class="line">  std::string <a class="code hl_variable" href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#ab6b7cf708ad9827476242237cfd1adb2">system_prompt</a>;                        <span class="comment">// Injected from resource catalogue</span></div>
<div class="line">};</div>
<div class="line"> </div>
<div class="line"><span class="keyword">class </span>OllamaAIService : <span class="keyword">public</span> AIService {</div>
<div class="line"> <span class="keyword">public</span>:</div>
<div class="line">  <span class="keyword">explicit</span> OllamaAIService(<span class="keyword">const</span> OllamaConfig&amp; config);</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Generate z3ed commands from natural language prompt</span></div>
<div class="line">  absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; <a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#ad2ae1595bd4640fd40390dc6edeaac4f">GetCommands</a>(</div>
<div class="line">      <span class="keyword">const</span> std::string&amp; prompt) <span class="keyword">override</span>;</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Health check: verify Ollama server is running and model is available</span></div>
<div class="line">  absl::Status <a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a082e4e2b00cec515e63305402bfaa9d3">CheckAvailability</a>();</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// List available models on Ollama server</span></div>
<div class="line">  absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; <a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a98fa7047f1cb3932bf94e078e0079d9f">ListAvailableModels</a>();</div>
<div class="line"> </div>
<div class="line"> <span class="keyword">private</span>:</div>
<div class="line">  OllamaConfig <a class="code hl_variable" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a29b0f3918b46647885a17e86bf9534e9">config_</a>;</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Build system prompt from resource catalogue</span></div>
<div class="line">  std::string <a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#ac6ac158faef88066545d0c001c2fb4bc">BuildSystemPrompt</a>();</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Parse JSON response from Ollama API</span></div>
<div class="line">  absl::StatusOr&lt;std::string&gt; <a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#aa47ce9da469009ae96b4109bb67df309">ParseOllamaResponse</a>(<span class="keyword">const</span> std::string&amp; json_response);</div>
<div class="line">};</div>
<div class="line"> </div>
<div class="line">}  <span class="comment">// namespace cli</span></div>
<div class="line">}  <span class="comment">// namespace yaze</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#endif  </span><span class="comment">// YAZE_SRC_CLI_OLLAMA_AI_SERVICE_H_</span></div>
<div class="ttc" id="aai__service_8h_html"><div class="ttname"><a href="../../d7/d52/ai__service_8h.html">ai_service.h</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_a082e4e2b00cec515e63305402bfaa9d3"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a082e4e2b00cec515e63305402bfaa9d3">yaze::cli::OllamaAIService::CheckAvailability</a></div><div class="ttdeci">absl::Status CheckAvailability()</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00053">ollama_ai_service.cc:53</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_a29b0f3918b46647885a17e86bf9534e9"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a29b0f3918b46647885a17e86bf9534e9">yaze::cli::OllamaAIService::config_</a></div><div class="ttdeci">OllamaConfig config_</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00040">ollama_ai_service.h:40</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_a98fa7047f1cb3932bf94e078e0079d9f"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#a98fa7047f1cb3932bf94e078e0079d9f">yaze::cli::OllamaAIService::ListAvailableModels</a></div><div class="ttdeci">absl::StatusOr&lt; std::vector&lt; std::string &gt; &gt; ListAvailableModels()</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00112">ollama_ai_service.cc:112</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_aa47ce9da469009ae96b4109bb67df309"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#aa47ce9da469009ae96b4109bb67df309">yaze::cli::OllamaAIService::ParseOllamaResponse</a></div><div class="ttdeci">absl::StatusOr&lt; std::string &gt; ParseOllamaResponse(const std::string &amp;json_response)</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00146">ollama_ai_service.cc:146</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_ac6ac158faef88066545d0c001c2fb4bc"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#ac6ac158faef88066545d0c001c2fb4bc">yaze::cli::OllamaAIService::BuildSystemPrompt</a></div><div class="ttdeci">std::string BuildSystemPrompt()</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00047">ollama_ai_service.cc:47</a></div></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_ad2ae1595bd4640fd40390dc6edeaac4f"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#ad2ae1595bd4640fd40390dc6edeaac4f">yaze::cli::OllamaAIService::GetCommands</a></div><div class="ttdeci">absl::StatusOr&lt; std::vector&lt; std::string &gt; &gt; GetCommands(const std::string &amp;prompt) override</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00167">ollama_ai_service.cc:167</a></div></div>
<div class="ttc" id="anamespaceyaze_html"><div class="ttname"><a href="../../dc/d46/namespaceyaze.html">yaze</a></div><div class="ttdoc">Main namespace for the application.</div><div class="ttdef"><b>Definition</b> <a href="../../db/d17/asar__wrapper_8cc_source.html#l00014">asar_wrapper.cc:14</a></div></div>
<div class="ttc" id="astructyaze_1_1cli_1_1OllamaConfig_html_aa219f4c0ee7685f810d8fdc2742ef19a"><div class="ttname"><a href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#aa219f4c0ee7685f810d8fdc2742ef19a">yaze::cli::OllamaConfig::max_tokens</a></div><div class="ttdeci">int max_tokens</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00020">ollama_ai_service.h:20</a></div></div>
<div class="ttc" id="astructyaze_1_1cli_1_1OllamaConfig_html_ab6b7cf708ad9827476242237cfd1adb2"><div class="ttname"><a href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#ab6b7cf708ad9827476242237cfd1adb2">yaze::cli::OllamaConfig::system_prompt</a></div><div class="ttdeci">std::string system_prompt</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00021">ollama_ai_service.h:21</a></div></div>
<div class="ttc" id="astructyaze_1_1cli_1_1OllamaConfig_html_aba1021dcaea2e27964f155388b5a6d55"><div class="ttname"><a href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#aba1021dcaea2e27964f155388b5a6d55">yaze::cli::OllamaConfig::temperature</a></div><div class="ttdeci">float temperature</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00019">ollama_ai_service.h:19</a></div></div>
<div class="ttc" id="astructyaze_1_1cli_1_1OllamaConfig_html_ae1bf7f1a0f72507b5e4da8794caece1f"><div class="ttname"><a href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#ae1bf7f1a0f72507b5e4da8794caece1f">yaze::cli::OllamaConfig::base_url</a></div><div class="ttdeci">std::string base_url</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00017">ollama_ai_service.h:17</a></div></div>
<div class="ttc" id="astructyaze_1_1cli_1_1OllamaConfig_html_afe9a8786f6353ee32bc6add2959db511"><div class="ttname"><a href="../../d0/d3d/structyaze_1_1cli_1_1OllamaConfig.html#afe9a8786f6353ee32bc6add2959db511">yaze::cli::OllamaConfig::model</a></div><div class="ttdeci">std::string model</div><div class="ttdef"><b>Definition</b> <a href="../../da/d1c/ollama__ai__service_8h_source.html#l00018">ollama_ai_service.h:18</a></div></div>
</div><!-- fragment --><p><b>File</b>: <code><a class="el" href="../../d9/d17/ollama__ai__service_8cc.html">src/cli/service/ollama_ai_service.cc</a></code></p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="../../da/d1c/ollama__ai__service_8h.html">cli/service/ollama_ai_service.h</a>&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &lt;cstdlib&gt;</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#include &quot;absl/strings/str_cat.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;absl/strings/str_format.h&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="preprocessor">#ifdef YAZE_WITH_HTTPLIB</span></div>
<div class="line"><span class="preprocessor">#include &quot;incl/httplib.h&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;third_party/json/src/json.hpp&quot;</span></div>
<div class="line"><span class="preprocessor">#endif</span></div>
<div class="line"> </div>
<div class="line"><span class="keyword">namespace </span><a class="code hl_namespace" href="../../dc/d46/namespaceyaze.html">yaze</a> {</div>
<div class="line"><span class="keyword">namespace </span>cli {</div>
<div class="line"> </div>
<div class="line"><a class="code hl_function" href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#aa54065448ee6fe182ad056d9099201b9">OllamaAIService::OllamaAIService</a>(<span class="keyword">const</span> OllamaConfig&amp; config) : config_(config) {</div>
<div class="line">  <span class="keywordflow">if</span> (config_.system_prompt.empty()) {</div>
<div class="line">    config_.system_prompt = BuildSystemPrompt();</div>
<div class="line">  }</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">std::string OllamaAIService::BuildSystemPrompt() {</div>
<div class="line">  <span class="keywordflow">return</span> R<span class="stringliteral">&quot;(You are an expert ROM hacking assistant for The Legend of Zelda: A Link to the Past.</span></div>
<div class="line"><span class="stringliteral">Your role is to generate PRECISE z3ed CLI commands to fulfill user requests.</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">CRITICAL RULES:</span></div>
<div class="line"><span class="stringliteral">1. Output ONLY a JSON array of command strings</span></div>
<div class="line"><span class="stringliteral">2. Each command must follow exact z3ed syntax</span></div>
<div class="line"><span class="stringliteral">3. Commands must be executable without modification</span></div>
<div class="line"><span class="stringliteral">4. Use only commands from the available command set</span></div>
<div class="line"><span class="stringliteral">5. Include all required arguments with proper flags</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">AVAILABLE COMMANDS:</span></div>
<div class="line"><span class="stringliteral">- rom info --rom &lt;path&gt;</span></div>
<div class="line"><span class="stringliteral">- rom validate --rom &lt;path&gt;</span></div>
<div class="line"><span class="stringliteral">- palette export --group &lt;group&gt; --id &lt;id&gt; --to &lt;file&gt;</span></div>
<div class="line"><span class="stringliteral">- palette import --group &lt;group&gt; --id &lt;id&gt; --from &lt;file&gt;</span></div>
<div class="line"><span class="stringliteral">- palette set-color --file &lt;file&gt; --index &lt;index&gt; --color &lt;hex_color&gt;</span></div>
<div class="line"><span class="stringliteral">- overworld get-tile --map &lt;map_id&gt; --x &lt;x&gt; --y &lt;y&gt;</span></div>
<div class="line"><span class="stringliteral">- overworld set-tile --map &lt;map_id&gt; --x &lt;x&gt; --y &lt;y&gt; --tile &lt;tile_id&gt;</span></div>
<div class="line"><span class="stringliteral">- dungeon export-room --room &lt;room_id&gt; --to &lt;file&gt;</span></div>
<div class="line"><span class="stringliteral">- dungeon import-room --room &lt;room_id&gt; --from &lt;file&gt;</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">RESPONSE FORMAT:</span></div>
<div class="line"><span class="stringliteral">[&quot;command1&quot;, &quot;command2&quot;, &quot;command3&quot;]</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">EXAMPLE USER REQUEST: &quot;Make all soldier armors red&quot;</span></div>
<div class="line"><span class="stringliteral">CORRECT RESPONSE:</span></div>
<div class="line"><span class="stringliteral">[&quot;palette export --group sprites --id soldier --to /tmp/soldier.pal&quot;,</span></div>
<div class="line"><span class="stringliteral"> &quot;palette set-color --file /tmp/soldier.pal --index 5 --color FF0000&quot;,</span></div>
<div class="line"><span class="stringliteral"> &quot;palette import --group sprites --id soldier --from /tmp/soldier.pal&quot;]</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">Begin your response now.)&quot;;</span></div>
<div class="line"><span class="stringliteral">}</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">absl::Status OllamaAIService::CheckAvailability() {</span></div>
<div class="line"><span class="stringliteral"></span><span class="preprocessor">#ifndef YAZE_WITH_HTTPLIB</span></div>
<div class="line">  <span class="keywordflow">return</span> absl::UnimplementedError(</div>
<div class="line">      <span class="stringliteral">&quot;Ollama service requires httplib. Build with vcpkg or system httplib.&quot;</span>);</div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line">  <span class="keywordflow">try</span> {</div>
<div class="line">    httplib::Client cli(config_.base_url);</div>
<div class="line">    cli.set_connection_timeout(5);  <span class="comment">// 5 second timeout</span></div>
<div class="line">    </div>
<div class="line">    <span class="keyword">auto</span> res = cli.Get(<span class="stringliteral">&quot;/api/tags&quot;</span>);</div>
<div class="line">    <span class="keywordflow">if</span> (!res) {</div>
<div class="line">      <span class="keywordflow">return</span> absl::UnavailableError(absl::StrFormat(</div>
<div class="line">          <span class="stringliteral">&quot;Cannot connect to Ollama server at %s. &quot;</span></div>
<div class="line">          <span class="stringliteral">&quot;Make sure Ollama is installed and running (ollama serve).&quot;</span>,</div>
<div class="line">          config_.base_url));</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">if</span> (res-&gt;status != 200) {</div>
<div class="line">      <span class="keywordflow">return</span> absl::InternalError(absl::StrFormat(</div>
<div class="line">          <span class="stringliteral">&quot;Ollama server error: HTTP %d&quot;</span>, res-&gt;status));</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Check if requested model is available</span></div>
<div class="line">    nlohmann::json models_json = nlohmann::json::parse(res-&gt;body);</div>
<div class="line">    <span class="keywordtype">bool</span> model_found = <span class="keyword">false</span>;</div>
<div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; model : models_json[<span class="stringliteral">&quot;models&quot;</span>]) {</div>
<div class="line">      <span class="keywordflow">if</span> (model[<span class="stringliteral">&quot;name&quot;</span>].get&lt;std::string&gt;().find(config_.model) != std::string::npos) {</div>
<div class="line">        model_found = <span class="keyword">true</span>;</div>
<div class="line">        <span class="keywordflow">break</span>;</div>
<div class="line">      }</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">if</span> (!model_found) {</div>
<div class="line">      <span class="keywordflow">return</span> absl::NotFoundError(absl::StrFormat(</div>
<div class="line">          <span class="stringliteral">&quot;Model &#39;%s&#39; not found. Pull it with: ollama pull %s&quot;</span>,</div>
<div class="line">          config_.model, config_.model));</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">return</span> absl::OkStatus();</div>
<div class="line">  } <span class="keywordflow">catch</span> (<span class="keyword">const</span> std::exception&amp; e) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::InternalError(absl::StrCat(<span class="stringliteral">&quot;Ollama check failed: &quot;</span>, e.what()));</div>
<div class="line">  }</div>
<div class="line"><span class="preprocessor">#endif</span></div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; OllamaAIService::ListAvailableModels() {</div>
<div class="line"><span class="preprocessor">#ifndef YAZE_WITH_HTTPLIB</span></div>
<div class="line">  <span class="keywordflow">return</span> absl::UnimplementedError(<span class="stringliteral">&quot;Requires httplib support&quot;</span>);</div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line">  httplib::Client cli(config_.base_url);</div>
<div class="line">  <span class="keyword">auto</span> res = cli.Get(<span class="stringliteral">&quot;/api/tags&quot;</span>);</div>
<div class="line">  </div>
<div class="line">  <span class="keywordflow">if</span> (!res || res-&gt;status != 200) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::UnavailableError(<span class="stringliteral">&quot;Cannot list Ollama models&quot;</span>);</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  nlohmann::json models_json = nlohmann::json::parse(res-&gt;body);</div>
<div class="line">  std::vector&lt;std::string&gt; models;</div>
<div class="line">  <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; model : models_json[<span class="stringliteral">&quot;models&quot;</span>]) {</div>
<div class="line">    models.push_back(model[<span class="stringliteral">&quot;name&quot;</span>].get&lt;std::string&gt;());</div>
<div class="line">  }</div>
<div class="line">  <span class="keywordflow">return</span> models;</div>
<div class="line"><span class="preprocessor">#endif</span></div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; OllamaAIService::GetCommands(</div>
<div class="line">    <span class="keyword">const</span> std::string&amp; prompt) {</div>
<div class="line"><span class="preprocessor">#ifndef YAZE_WITH_HTTPLIB</span></div>
<div class="line">  <span class="keywordflow">return</span> absl::UnimplementedError(</div>
<div class="line">      <span class="stringliteral">&quot;Ollama service requires httplib. Build with vcpkg or system httplib.&quot;</span>);</div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Build request payload</span></div>
<div class="line">  nlohmann::json request_body = {</div>
<div class="line">    {<span class="stringliteral">&quot;model&quot;</span>, config_.model},</div>
<div class="line">    {<span class="stringliteral">&quot;prompt&quot;</span>, config_.system_prompt + <span class="stringliteral">&quot;\n\nUSER REQUEST: &quot;</span> + prompt},</div>
<div class="line">    {<span class="stringliteral">&quot;stream&quot;</span>, <span class="keyword">false</span>},</div>
<div class="line">    {<span class="stringliteral">&quot;temperature&quot;</span>, config_.temperature},</div>
<div class="line">    {<span class="stringliteral">&quot;max_tokens&quot;</span>, config_.max_tokens},</div>
<div class="line">    {<span class="stringliteral">&quot;format&quot;</span>, <span class="stringliteral">&quot;json&quot;</span>}  <span class="comment">// Force JSON output</span></div>
<div class="line">  };</div>
<div class="line">  </div>
<div class="line">  httplib::Client cli(config_.base_url);</div>
<div class="line">  cli.set_read_timeout(60);  <span class="comment">// Longer timeout for inference</span></div>
<div class="line">  </div>
<div class="line">  <span class="keyword">auto</span> res = cli.Post(<span class="stringliteral">&quot;/api/generate&quot;</span>, request_body.dump(), <span class="stringliteral">&quot;application/json&quot;</span>);</div>
<div class="line">  </div>
<div class="line">  <span class="keywordflow">if</span> (!res) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::UnavailableError(</div>
<div class="line">        <span class="stringliteral">&quot;Failed to connect to Ollama. Is &#39;ollama serve&#39; running?&quot;</span>);</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="keywordflow">if</span> (res-&gt;status != 200) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::InternalError(absl::StrFormat(</div>
<div class="line">        <span class="stringliteral">&quot;Ollama API error: HTTP %d - %s&quot;</span>, res-&gt;status, res-&gt;body));</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Parse response</span></div>
<div class="line">  <span class="keywordflow">try</span> {</div>
<div class="line">    nlohmann::json response_json = nlohmann::json::parse(res-&gt;body);</div>
<div class="line">    std::string generated_text = response_json[<span class="stringliteral">&quot;response&quot;</span>].get&lt;std::string&gt;();</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Parse the command array from generated text</span></div>
<div class="line">    nlohmann::json commands_json = nlohmann::json::parse(generated_text);</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">if</span> (!commands_json.is_array()) {</div>
<div class="line">      <span class="keywordflow">return</span> absl::InvalidArgumentError(</div>
<div class="line">          <span class="stringliteral">&quot;LLM did not return a JSON array. Response: &quot;</span> + generated_text);</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    std::vector&lt;std::string&gt; commands;</div>
<div class="line">    <span class="keywordflow">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; cmd : commands_json) {</div>
<div class="line">      <span class="keywordflow">if</span> (cmd.is_string()) {</div>
<div class="line">        commands.push_back(cmd.get&lt;std::string&gt;());</div>
<div class="line">      }</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">if</span> (commands.empty()) {</div>
<div class="line">      <span class="keywordflow">return</span> absl::InvalidArgumentError(</div>
<div class="line">          <span class="stringliteral">&quot;LLM returned empty command list. Prompt may be unclear.&quot;</span>);</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    <span class="keywordflow">return</span> commands;</div>
<div class="line">    </div>
<div class="line">  } <span class="keywordflow">catch</span> (<span class="keyword">const</span> nlohmann::json::exception&amp; e) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::InternalError(absl::StrCat(</div>
<div class="line">        <span class="stringliteral">&quot;Failed to parse Ollama response: &quot;</span>, e.what(), <span class="stringliteral">&quot;\nRaw: &quot;</span>, res-&gt;body));</div>
<div class="line">  }</div>
<div class="line"><span class="preprocessor">#endif</span></div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line">}  <span class="comment">// namespace cli</span></div>
<div class="line">}  <span class="comment">// namespace yaze</span></div>
<div class="ttc" id="aclassyaze_1_1cli_1_1OllamaAIService_html_aa54065448ee6fe182ad056d9099201b9"><div class="ttname"><a href="../../dc/d1d/classyaze_1_1cli_1_1OllamaAIService.html#aa54065448ee6fe182ad056d9099201b9">yaze::cli::OllamaAIService::OllamaAIService</a></div><div class="ttdeci">OllamaAIService(const OllamaConfig &amp;config)</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d17/ollama__ai__service_8cc_source.html#l00033">ollama_ai_service.cc:33</a></div></div>
<div class="ttc" id="aollama__ai__service_8h_html"><div class="ttname"><a href="../../da/d1c/ollama__ai__service_8h.html">ollama_ai_service.h</a></div></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md1625"></a>
1.2. Add CMake Configuration</h3>
<p><b>File</b>: <code>CMakeLists.txt</code> (add to dependencies section)</p>
<div class="fragment"><div class="line"># Optional httplib for AI services (Ollama, Gemini)</div>
<div class="line">option(YAZE_WITH_HTTPLIB &quot;Enable HTTP client for AI services&quot; ON)</div>
<div class="line"> </div>
<div class="line">if(YAZE_WITH_HTTPLIB)</div>
<div class="line">  find_package(httplib CONFIG)</div>
<div class="line">  if(httplib_FOUND)</div>
<div class="line">    set(YAZE_WITH_HTTPLIB ON)</div>
<div class="line">    add_compile_definitions(YAZE_WITH_HTTPLIB)</div>
<div class="line">    message(STATUS &quot;httplib found - AI services enabled&quot;)</div>
<div class="line">  else()</div>
<div class="line">    # Try to use bundled httplib from third_party</div>
<div class="line">    if(EXISTS &quot;${CMAKE_SOURCE_DIR}/third_party/httplib&quot;)</div>
<div class="line">      set(YAZE_WITH_HTTPLIB ON)</div>
<div class="line">      add_compile_definitions(YAZE_WITH_HTTPLIB)</div>
<div class="line">      message(STATUS &quot;Using bundled httplib - AI services enabled&quot;)</div>
<div class="line">    else()</div>
<div class="line">      set(YAZE_WITH_HTTPLIB OFF)</div>
<div class="line">      message(WARNING &quot;httplib not found - AI services disabled&quot;)</div>
<div class="line">    endif()</div>
<div class="line">  endif()</div>
<div class="line">endif()</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md1626"></a>
1.3. Wire into Agent Commands</h3>
<p><b>File</b>: <code><a class="el" href="../../d9/d8d/general__commands_8cc.html">src/cli/handlers/agent/general_commands.cc</a></code></p>
<p>Replace hardcoded <code>MockAIService</code> usage with service selection:</p>
<div class="fragment"><div class="line"><span class="preprocessor">#include &quot;<a class="code" href="../../da/d1c/ollama__ai__service_8h.html">cli/service/ollama_ai_service.h</a>&quot;</span></div>
<div class="line"><span class="preprocessor">#include &quot;<a class="code" href="../../d3/d22/gemini__ai__service_8h.html">cli/service/gemini_ai_service.h</a>&quot;</span></div>
<div class="line"> </div>
<div class="line"><span class="comment">// Helper: Select AI service based on environment</span></div>
<div class="line">std::unique_ptr&lt;AIService&gt; CreateAIService() {</div>
<div class="line">  <span class="comment">// Priority: Ollama (local) &gt; Gemini (remote) &gt; Mock (testing)</span></div>
<div class="line">  </div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">char</span>* ollama_env = std::getenv(<span class="stringliteral">&quot;YAZE_AI_PROVIDER&quot;</span>);</div>
<div class="line">  <span class="keyword">const</span> <span class="keywordtype">char</span>* gemini_key = std::getenv(<span class="stringliteral">&quot;GEMINI_API_KEY&quot;</span>);</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Explicit provider selection</span></div>
<div class="line">  <span class="keywordflow">if</span> (ollama_env &amp;&amp; std::string(ollama_env) == <span class="stringliteral">&quot;ollama&quot;</span>) {</div>
<div class="line">    OllamaConfig config;</div>
<div class="line">    <span class="comment">// Allow model override via env</span></div>
<div class="line">    <span class="keywordflow">if</span> (<span class="keyword">const</span> <span class="keywordtype">char</span>* model = std::getenv(<span class="stringliteral">&quot;OLLAMA_MODEL&quot;</span>)) {</div>
<div class="line">      config.model = model;</div>
<div class="line">    }</div>
<div class="line">    <span class="keyword">auto</span> service = std::make_unique&lt;OllamaAIService&gt;(config);</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Health check</span></div>
<div class="line">    <span class="keywordflow">if</span> (<span class="keyword">auto</span> status = service-&gt;CheckAvailability(); !status.ok()) {</div>
<div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;⚠️  Ollama unavailable: &quot;</span> &lt;&lt; status.message() &lt;&lt; std::endl;</div>
<div class="line">      std::cerr &lt;&lt; <span class="stringliteral">&quot;   Falling back to MockAIService&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">      <span class="keywordflow">return</span> std::make_unique&lt;MockAIService&gt;();</div>
<div class="line">    }</div>
<div class="line">    </div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;🤖 Using Ollama AI with model: &quot;</span> &lt;&lt; config.model &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">return</span> service;</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Gemini if API key provided</span></div>
<div class="line">  <span class="keywordflow">if</span> (gemini_key &amp;&amp; std::strlen(gemini_key) &gt; 0) {</div>
<div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;🤖 Using Gemini AI (remote)&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">    <span class="keywordflow">return</span> std::make_unique&lt;GeminiAIService&gt;(gemini_key);</div>
<div class="line">  }</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// Default: Mock service for testing</span></div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;🤖 Using MockAIService (no LLM configured)&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  std::cout &lt;&lt; <span class="stringliteral">&quot;   Set YAZE_AI_PROVIDER=ollama or GEMINI_API_KEY to enable LLM&quot;</span> &lt;&lt; std::endl;</div>
<div class="line">  <span class="keywordflow">return</span> std::make_unique&lt;MockAIService&gt;();</div>
<div class="line">}</div>
<div class="line"> </div>
<div class="line"><span class="comment">// Update HandleRunCommand:</span></div>
<div class="line">absl::Status <a class="code hl_function" href="../../d8/dd3/namespaceyaze_1_1cli_1_1agent.html#a4c88dbe6c96f20bba8fd432738e1a528">HandleRunCommand</a>(<span class="keyword">const</span> std::vector&lt;std::string&gt;&amp; arg_vec) {</div>
<div class="line">  <span class="comment">// ... existing setup code ...</span></div>
<div class="line">  </div>
<div class="line">  <span class="keyword">auto</span> ai_service = <a class="code hl_function" href="../../d1/ddf/namespaceyaze_1_1cli_1_1agent_1_1anonymous__namespace_02general__commands_8cc_03.html#a3b384fcc92873946c5a14e930978d3f7">CreateAIService</a>();  <span class="comment">// ← Replace MockAIService instantiation</span></div>
<div class="line">  <span class="keyword">auto</span> commands_or = ai_service-&gt;GetCommands(prompt);</div>
<div class="line">  </div>
<div class="line">  <span class="comment">// ... rest of execution logic ...</span></div>
<div class="line">}</div>
<div class="ttc" id="agemini__ai__service_8h_html"><div class="ttname"><a href="../../d3/d22/gemini__ai__service_8h.html">gemini_ai_service.h</a></div></div>
<div class="ttc" id="anamespaceyaze_1_1cli_1_1agent_1_1anonymous__namespace_02general__commands_8cc_03_html_a3b384fcc92873946c5a14e930978d3f7"><div class="ttname"><a href="../../d1/ddf/namespaceyaze_1_1cli_1_1agent_1_1anonymous__namespace_02general__commands_8cc_03.html#a3b384fcc92873946c5a14e930978d3f7">yaze::cli::agent::anonymous_namespace{general_commands.cc}::CreateAIService</a></div><div class="ttdeci">std::unique_ptr&lt; AIService &gt; CreateAIService()</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d8d/general__commands_8cc_source.html#l00040">general_commands.cc:40</a></div></div>
<div class="ttc" id="anamespaceyaze_1_1cli_1_1agent_html_a4c88dbe6c96f20bba8fd432738e1a528"><div class="ttname"><a href="../../d8/dd3/namespaceyaze_1_1cli_1_1agent.html#a4c88dbe6c96f20bba8fd432738e1a528">yaze::cli::agent::HandleRunCommand</a></div><div class="ttdeci">absl::Status HandleRunCommand(const std::vector&lt; std::string &gt; &amp;args, Rom &amp;rom)</div><div class="ttdef"><b>Definition</b> <a href="../../d9/d8d/general__commands_8cc_source.html#l00165">general_commands.cc:165</a></div></div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md1627"></a>
1.4. Testing &amp; Validation</h3>
<p><b>Prerequisites</b>: </p><div class="fragment"><div class="line"># Install Ollama (macOS)</div>
<div class="line">brew install ollama</div>
<div class="line"> </div>
<div class="line"># Start Ollama server</div>
<div class="line">ollama serve &amp;</div>
<div class="line"> </div>
<div class="line"># Pull recommended model</div>
<div class="line">ollama pull qwen2.5-coder:7b</div>
<div class="line"> </div>
<div class="line"># Test connectivity</div>
<div class="line">curl http://localhost:11434/api/tags</div>
</div><!-- fragment --><p><b>End-to-End Test Script</b> (<code>scripts/test_ollama_integration.sh</code>):</p>
<div class="fragment"><div class="line">#!/bin/bash</div>
<div class="line">set -e</div>
<div class="line"> </div>
<div class="line">echo &quot;🧪 Testing Ollama AI Integration&quot;</div>
<div class="line"> </div>
<div class="line"># 1. Check Ollama availability</div>
<div class="line">echo &quot;Checking Ollama server...&quot;</div>
<div class="line">if ! curl -s http://localhost:11434/api/tags &gt; /dev/null; then</div>
<div class="line">  echo &quot;❌ Ollama not running. Start with: ollama serve&quot;</div>
<div class="line">  exit 1</div>
<div class="line">fi</div>
<div class="line"> </div>
<div class="line"># 2. Check model availability</div>
<div class="line">echo &quot;Checking qwen2.5-coder:7b model...&quot;</div>
<div class="line">if ! ollama list | grep -q &quot;qwen2.5-coder:7b&quot;; then</div>
<div class="line">  echo &quot;⚠️  Model not found. Pulling...&quot;</div>
<div class="line">  ollama pull qwen2.5-coder:7b</div>
<div class="line">fi</div>
<div class="line"> </div>
<div class="line"># 3. Test agent run with simple prompt</div>
<div class="line">echo &quot;Testing agent run command...&quot;</div>
<div class="line">export YAZE_AI_PROVIDER=ollama</div>
<div class="line">export OLLAMA_MODEL=qwen2.5-coder:7b</div>
<div class="line"> </div>
<div class="line">./build/bin/z3ed agent run \</div>
<div class="line">  --prompt &quot;Export the first overworld palette to /tmp/test.pal&quot; \</div>
<div class="line">  --rom zelda3.sfc \</div>
<div class="line">  --sandbox</div>
<div class="line"> </div>
<div class="line"># 4. Verify proposal created</div>
<div class="line">echo &quot;Checking proposal registry...&quot;</div>
<div class="line">if ! ./build/bin/z3ed agent list | grep -q &quot;pending&quot;; then</div>
<div class="line">  echo &quot;❌ No pending proposal found&quot;</div>
<div class="line">  exit 1</div>
<div class="line">fi</div>
<div class="line"> </div>
<div class="line"># 5. Review generated commands</div>
<div class="line">echo &quot;✅ Reviewing generated commands...&quot;</div>
<div class="line">./build/bin/z3ed agent diff --format yaml</div>
<div class="line"> </div>
<div class="line">echo &quot;✅ Ollama integration test passed!&quot;</div>
</div><!-- fragment --><hr  />
<h2><a class="anchor" id="autotoc_md1629"></a>
Phase 2: Improve Gemini Integration (2-3 hours)</h2>
<p>The existing <code>GeminiAIService</code> needs fixes and better prompting:</p>
<h3><a class="anchor" id="autotoc_md1630"></a>
2.1. Fix GeminiAIService Implementation</h3>
<p><b>File</b>: <code><a class="el" href="../../df/df3/gemini__ai__service_8cc.html">src/cli/service/gemini_ai_service.cc</a></code></p>
<div class="fragment"><div class="line">absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; GeminiAIService::GetCommands(</div>
<div class="line">    <span class="keyword">const</span> std::string&amp; prompt) {</div>
<div class="line"><span class="preprocessor">#ifndef YAZE_WITH_HTTPLIB</span></div>
<div class="line">  <span class="keywordflow">return</span> absl::UnimplementedError(</div>
<div class="line">      <span class="stringliteral">&quot;Gemini AI service requires httplib. Build with vcpkg.&quot;</span>);</div>
<div class="line"><span class="preprocessor">#else</span></div>
<div class="line">  <span class="keywordflow">if</span> (api_key_.empty()) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::FailedPreconditionError(</div>
<div class="line">        <span class="stringliteral">&quot;GEMINI_API_KEY not set. Get key from: https://makersuite.google.com/app/apikey&quot;</span>);</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// Build comprehensive system instruction</span></div>
<div class="line">  std::string system_instruction = R<span class="stringliteral">&quot;({</span></div>
<div class="line"><span class="stringliteral">    &quot;role&quot;: &quot;system&quot;,</span></div>
<div class="line"><span class="stringliteral">    &quot;content&quot;: &quot;You are an expert ROM hacking assistant for The Legend of Zelda: A Link to the Past. Generate ONLY a JSON array of z3ed CLI commands. Each command must be executable without modification. Available commands: rom info, rom validate, palette export/import/set-color, overworld get-tile/set-tile, dungeon export-room/import-room. Response format: [\&quot;command1\&quot;, \&quot;command2\&quot;]&quot;</span></div>
<div class="line"><span class="stringliteral">  })&quot;;</span></div>
<div class="line"><span class="stringliteral"></span> </div>
<div class="line"><span class="stringliteral">  httplib::Client cli(</span><span class="stringliteral">&quot;https://generativelanguage.googleapis.com&quot;</span>);</div>
<div class="line">  cli.set_read_timeout(60);</div>
<div class="line">  </div>
<div class="line">  nlohmann::json request_body = {</div>
<div class="line">    {<span class="stringliteral">&quot;contents&quot;</span>, {{</div>
<div class="line">      {<span class="stringliteral">&quot;role&quot;</span>, <span class="stringliteral">&quot;user&quot;</span>},</div>
<div class="line">      {<span class="stringliteral">&quot;parts&quot;</span>, {{</div>
<div class="line">        {<span class="stringliteral">&quot;text&quot;</span>, absl::StrFormat(<span class="stringliteral">&quot;System: %s\n\nUser: %s&quot;</span>, </div>
<div class="line">                                 system_instruction, prompt)}</div>
<div class="line">      }}}</div>
<div class="line">    }}},</div>
<div class="line">    {<span class="stringliteral">&quot;generationConfig&quot;</span>, {</div>
<div class="line">      {<span class="stringliteral">&quot;temperature&quot;</span>, 0.1},          <span class="comment">// Low temp for deterministic output</span></div>
<div class="line">      {<span class="stringliteral">&quot;maxOutputTokens&quot;</span>, 2048},</div>
<div class="line">      {<span class="stringliteral">&quot;topP&quot;</span>, 0.8},</div>
<div class="line">      {<span class="stringliteral">&quot;topK&quot;</span>, 10}</div>
<div class="line">    }},</div>
<div class="line">    {<span class="stringliteral">&quot;safetySettings&quot;</span>, {</div>
<div class="line">      {{<span class="stringliteral">&quot;category&quot;</span>, <span class="stringliteral">&quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;</span>}, {<span class="stringliteral">&quot;threshold&quot;</span>, <span class="stringliteral">&quot;BLOCK_NONE&quot;</span>}}</div>
<div class="line">    }}</div>
<div class="line">  };</div>
<div class="line"> </div>
<div class="line">  httplib::Headers headers = {</div>
<div class="line">      {<span class="stringliteral">&quot;Content-Type&quot;</span>, <span class="stringliteral">&quot;application/json&quot;</span>},</div>
<div class="line">  };</div>
<div class="line">  </div>
<div class="line">  std::string endpoint = absl::StrFormat(</div>
<div class="line">      <span class="stringliteral">&quot;/v1beta/models/gemini-2.5-flash:generateContent?key=%s&quot;</span>, api_key_);</div>
<div class="line"> </div>
<div class="line">  <span class="keyword">auto</span> res = cli.Post(endpoint, headers, request_body.dump(), <span class="stringliteral">&quot;application/json&quot;</span>);</div>
<div class="line"> </div>
<div class="line">  <span class="keywordflow">if</span> (!res) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::UnavailableError(</div>
<div class="line">        <span class="stringliteral">&quot;Failed to connect to Gemini API. Check internet connection.&quot;</span>);</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  <span class="keywordflow">if</span> (res-&gt;status != 200) {</div>
<div class="line">    <span class="keywordflow">return</span> absl::InternalError(absl::StrFormat(</div>
<div class="line">        <span class="stringliteral">&quot;Gemini API error: HTTP %d - %s&quot;</span>, res-&gt;status, res-&gt;body));</div>
<div class="line">  }</div>
<div class="line"> </div>
<div class="line">  <span class="comment">// Parse response</span></div>
<div class="line">  <span class="keywordflow">try</span> {</div>
<div class="line">    nlohmann::json response_json = nlohmann::json::parse(res-&gt;body);</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Extract text from nested structure</span></div>
<div class="line">    std::string text_content = </div>
<div class="line">        response_json[<span class="stringliteral">&quot;candidates&quot;</span>][0][<span class="stringliteral">&quot;content&quot;</span>][<span class="stringliteral">&quot;parts&quot;</span>][0][<span class="stringliteral">&quot;text&quot;</span>]</div>
<div class="line">        .get&lt;std::string&gt;();</div>
<div class="line">    </div>
<div class="line">    <span class="comment">// Gemini may wrap JSON in markdown code blocks - strip them</span></div>
<div class="line">    <span class="keywordflow">if</span> (text_content.find(<span class="stringliteral">&quot;</span></div>
</div><!-- fragment --><p> json") != std::string::npos) {
      size_t start = text_content.find("[");
      size_t end = text_content.rfind("]");
      if (start != std::string::npos &amp;&amp; end != std::string::npos) {
        text_content = text_content.substr(start, end - start + 1);
      }
    }

    nlohmann::json commands_array = nlohmann::json::parse(text_content);

    if (!commands_array.is_array()) {
      return absl::InvalidArgumentError(
          "Gemini did not return a JSON array. Response: " + text_content);
    }

    std::vector&lt;std::string&gt; commands;
    for (const auto&amp; cmd : commands_array) {
      if (cmd.is_string()) {
        commands.push_back(cmd.get&lt;std::string&gt;());
      }
    }

    return commands;

  } catch (const nlohmann::json::exception&amp; e) {
    return absl::InternalError(absl::StrCat(
        "Failed to parse Gemini response: ", e.what(), "\nRaw: ", res-&gt;body));
  }
#endif
}
@icode 

---

### Phase 3: Add Claude Integration (2-3 hours)

Claude 3.5 Sonnet is excellent for code generation and has a generous free tier.

#### 3.1. Create ClaudeAIService

**File**: `src/cli/service/claude_ai_service.h`

@endicode cpp
#ifndef YAZE_SRC_CLI_CLAUDE_AI_SERVICE_H_
#define YAZE_SRC_CLI_CLAUDE_AI_SERVICE_H_

#include &lt;string&gt;
#include &lt;vector&gt;

#include "absl/status/statusor.h"
#include "<a class="el" href="../../d7/d52/ai__service_8h.html">cli/service/ai_service.h</a>"

namespace yaze {
namespace cli {

class ClaudeAIService : public AIService {
 public:
  explicit ClaudeAIService(const std::string&amp; api_key);

  absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; GetCommands(
      const std::string&amp; prompt) override;

 private:
  std::string api_key_;
  std::string model_ = "claude-3-5-sonnet-20241022";  // Latest version
};

}  // namespace cli
}  // namespace yaze

#endif  // YAZE_SRC_CLI_CLAUDE_AI_SERVICE_H_
@icode 

**File**: `src/cli/service/claude_ai_service.cc`

@endicode cpp
#include "cli/service/claude_ai_service.h"

#include "absl/strings/str_format.h"

#ifdef YAZE_WITH_HTTPLIB
#include "incl/httplib.h"
#include "third_party/json/src/json.hpp"
#endif

namespace yaze {
namespace cli {

ClaudeAIService::ClaudeAIService(const std::string&amp; api_key) 
    : api_key_(api_key) {}

absl::StatusOr&lt;std::vector&lt;std::string&gt;&gt; ClaudeAIService::GetCommands(
    const std::string&amp; prompt) {
#ifndef YAZE_WITH_HTTPLIB
  return absl::UnimplementedError("Claude service requires httplib");
#else
  if (api_key_.empty()) {
    return absl::FailedPreconditionError(
        "CLAUDE_API_KEY not set. Get key from: <a href="https://console.anthropic.com/">https://console.anthropic.com/</a>");
  }

  httplib::Client cli("<a href="https://api.anthropic.com">https://api.anthropic.com</a>");
  cli.set_read_timeout(60);

  nlohmann::json request_body = {
    {"model", model_},
    {"max_tokens", 2048},
    {"temperature", 0.1},
    {"system", "You are an expert ROM hacking assistant. Generate ONLY a JSON array of z3ed commands. No explanations."},
    {"messages", {{
      {"role", "user"},
      {"content", prompt}
    }}}
  };

  httplib::Headers headers = {
    {"Content-Type", "application/json"},
    {"x-api-key", api_key_},
    {"anthropic-version", "2023-06-01"}
  };

  auto res = cli.Post("/v1/messages", headers, request_body.dump(), 
                      "application/json");

  if (!res) {
    return absl::UnavailableError("Failed to connect to Claude API");
  }

  if (res-&gt;status != 200) {
    return absl::InternalError(absl::StrFormat(
        "Claude API error: HTTP d - s", res-&gt;status, res-&gt;body));
  }

  try {
    nlohmann::json response_json = nlohmann::json::parse(res-&gt;body);
    std::string text_content = 
        response_json["content"][0]["text"].get&lt;std::string&gt;();

    // Claude may wrap in markdown - strip if present
    if (text_content.find("```json") != std::string::npos) {
      size_t start = text_content.find("[");
      size_t end = text_content.rfind("]");
      if (start != std::string::npos &amp;&amp; end != std::string::npos) {
        text_content = text_content.substr(start, end - start + 1);
      }
    }

    nlohmann::json commands_json = nlohmann::json::parse(text_content);

    std::vector&lt;std::string&gt; commands;
    for (const auto&amp; cmd : commands_json) {
      if (cmd.is_string()) {
        commands.push_back(cmd.get&lt;std::string&gt;());
      }
    }

    return commands;

  } catch (const std::exception&amp; e) {
    return absl::InternalError(absl::StrCat(
        "Failed to parse Claude response: ", e.what()));
  }
#endif
}

}  // namespace cli
}  // namespace yaze
@icode 

---

### Phase 4: Enhanced Prompt Engineering (3-4 hours)

#### 4.1. Load Resource Catalogue into System Prompt

**File**: `src/cli/service/prompt_builder.h`

@endicode cpp
#ifndef YAZE_SRC_CLI_PROMPT_BUILDER_H_
#define YAZE_SRC_CLI_PROMPT_BUILDER_H_

#include &lt;string&gt;
#include "absl/status/statusor.h"

namespace yaze {
namespace cli {

// Utility for building comprehensive LLM prompts from resource catalogue
class PromptBuilder {
 public:
  // Load command schemas from docs/api/z3ed-resources.yaml
  static absl::StatusOr&lt;std::string&gt; LoadResourceCatalogue();

  // Build system prompt with full command documentation
  static std::string BuildSystemPrompt();

  // Build few-shot examples for better LLM performance
  static std::string BuildFewShotExamples();

  // Inject ROM context (current ROM info, loaded editors, etc.)
  static std::string BuildContextPrompt();
};

}  // namespace cli
}  // namespace yaze

#endif  // YAZE_SRC_CLI_PROMPT_BUILDER_H_
@icode 

#### 4.2. Few-Shot Examples

Include proven examples in system prompt:

@endicode cpp
std::string PromptBuilder::BuildFewShotExamples() {
  return R"( EXAMPLE 1: User: "Make soldier armor red" Response: ["palette export --group sprites --id soldier --to /tmp/soldier.pal", "palette set-color --file /tmp/soldier.pal --index 5 --color FF0000", "palette import --group sprites --id soldier --from /tmp/soldier.pal"]</p>
<p>EXAMPLE 2: User: "Validate ROM integrity" Response: ["rom validate --rom zelda3.sfc"]</p>
<p>EXAMPLE 3: User: "Change tile at coordinates (10, 20) in Light World to grass" Response: ["overworld set-tile --map 0 --x 10 --y 20 --tile 0x40"] )";
}
@icode 

---

## 2. Configuration &amp; User Experience

### Environment Variables

@endicode bash
@section autotoc_md1631 AI Provider Selection
export YAZE_AI_PROVIDER=ollama    # Options: ollama, gemini, claude, mock
export OLLAMA_MODEL=qwen2.5-coder:7b
export OLLAMA_URL=http://localhost:11434

@section autotoc_md1632 API Keys (remote providers)
export GEMINI_API_KEY=your_key_here
export CLAUDE_API_KEY=your_key_here

@section autotoc_md1633 Logging &amp; Debugging
export YAZE_AI_DEBUG=1            # Log full prompts and responses
export YAZE_AI_CACHE_DIR=/tmp/yaze_ai_cache  # Cache LLM responses
@icode 

### CLI Flags

Add new flags to `z3ed agent run`:

@endicode bash
@section autotoc_md1634 Override provider for single command
z3ed agent run &amp;ndash;prompt "..." &amp;ndash;ai-provider ollama

@section autotoc_md1635 Override model
z3ed agent run &amp;ndash;prompt "..." &amp;ndash;ai-model "llama3:70b"

@section autotoc_md1636 Dry run: show generated commands without executing
z3ed agent run &amp;ndash;prompt "..." &amp;ndash;dry-run

@section autotoc_md1637 Interactive mode: confirm each command before execution
z3ed agent run &amp;ndash;prompt "..." &amp;ndash;interactive
@icode 

---

## 3. Testing &amp; Validation

### Unit Tests

**File**: `test/cli/ai_service_test.cc`

@endicode cpp
#include "<a class="el" href="../../da/d1c/ollama__ai__service_8h.html">cli/service/ollama_ai_service.h</a>"
#include "<a class="el" href="../../d3/d22/gemini__ai__service_8h.html">cli/service/gemini_ai_service.h</a>"
#include "cli/service/claude_ai_service.h"
#include &lt;gtest/gtest.h&gt;

TEST(OllamaAIServiceTest, CheckAvailability) {
  OllamaConfig config;
  config.base_url = "<a href="http://localhost:11434">http://localhost:11434</a>";
  OllamaAIService service(config);

  // Should not crash, may return unavailable if Ollama not running
  auto status = service.CheckAvailability();
  EXPECT_TRUE(status.ok() || 
              absl::IsUnavailable(status) || 
              absl::IsNotFound(status));
}

TEST(OllamaAIServiceTest, GetCommands) {
  // Skip if Ollama not available
  OllamaConfig config;
  OllamaAIService service(config);
  if (!service.CheckAvailability().ok()) {
    GTEST_SKIP() &lt;&lt; "Ollama not available";
  }

  auto result = service.GetCommands("Validate the ROM");
  ASSERT_TRUE(result.ok()) &lt;&lt; result.status();

  auto commands = result.value();
  EXPECT_GT(commands.size(), 0);
  EXPECT_THAT(commands[0], testing::HasSubstr("rom validate"));
}
@icode 

### Integration Tests

**File**: `scripts/test_ai_services.sh`

@endicode bash
#!/bin/bash
set -e

echo "🧪 Testing AI Services Integration"

@section autotoc_md1638 Test 1: Ollama (if available)
if curl -s http://localhost:11434/api/tags &gt; /dev/null 2&gt;&amp;1; then
  echo "✓ Ollama available - testing..."
  export YAZE_AI_PROVIDER=ollama
  ./build/bin/z3ed agent plan &amp;ndash;prompt "Export first palette"
else
  echo "⊘ Ollama not running - skipping"
fi

@section autotoc_md1639 Test 2: Gemini (if key set)
if [ -n "$GEMINI_API_KEY" ]; then
  echo "✓ Gemini API key set - testing..."
  export YAZE_AI_PROVIDER=gemini
  ./build/bin/z3ed agent plan &amp;ndash;prompt "Validate ROM"
else
  echo "⊘ GEMINI_API_KEY not set - skipping"
fi

@section autotoc_md1640 Test 3: Claude (if key set)
if [ -n "$CLAUDE_API_KEY" ]; then
  echo "✓ Claude API key set - testing..."
  export YAZE_AI_PROVIDER=claude
  ./build/bin/z3ed agent plan &amp;ndash;prompt "Export dungeon room"
else
  echo "⊘ CLAUDE_API_KEY not set - skipping"
fi

echo "✅ All available AI services tested successfully"
@icode 

---

## 4. Documentation Updates

### User Guide

**File**: `docs/z3ed/AI-SERVICE-SETUP.md`

@endicode markdown
@section autotoc_md1641 Setting Up LLM Integration for z3ed

@subsection autotoc_md1642 Quick Start: Ollama (Recommended)

1. &lt;strong&gt;Install Ollama&lt;/strong&gt;:
   @icode{bash} 
   # macOS
   brew install ollama
   
   # Linux
   curl -fsSL https://ollama.com/install.sh | sh
   @endicode 

2. &lt;strong&gt;Start Server&lt;/strong&gt;:
   @icode{bash} 
   ollama serve
   @endicode 

3. &lt;strong&gt;Pull Model&lt;/strong&gt;:
   @icode{bash} 
   ollama pull qwen2.5-coder:7b  # Recommended: fast + accurate
   @endicode 

4. &lt;strong&gt;Configure z3ed&lt;/strong&gt;:
   @icode{bash} 
   export YAZE_AI_PROVIDER=ollama
   @endicode 

5. &lt;strong&gt;Test&lt;/strong&gt;:
   @icode{bash} 
   z3ed agent run --prompt "Validate my ROM" --rom zelda3.sfc
   @endicode 

@subsection autotoc_md1643 Alternative: Gemini API (Remote)

1. Get API key: https://makersuite.google.com/app/apikey
2. Configure:
   @icode{bash} 
   export GEMINI_API_KEY=your_key_here
   export YAZE_AI_PROVIDER=gemini
   @endicode 
3. Run: &lt;tt&gt;z3ed agent run --prompt "..."&lt;/tt&gt;

@subsection autotoc_md1644 Alternative: Claude API (Remote)

1. Get API key: https://console.anthropic.com/
2. Configure:
   @icode{bash} 
   export CLAUDE_API_KEY=your_key_here
   export YAZE_AI_PROVIDER=claude
   @endicode 
3. Run: &lt;tt&gt;z3ed agent run --prompt "..."&lt;/tt&gt;

@subsection autotoc_md1645 Troubleshooting

&lt;strong&gt;Issue&lt;/strong&gt;: "Cannot connect to Ollama"  &lt;br&gt;
&lt;strong&gt;Solution&lt;/strong&gt;: Make sure &lt;tt&gt;ollama serve&lt;/tt&gt; is running

&lt;strong&gt;Issue&lt;/strong&gt;: "Model not found"  &lt;br&gt;
&lt;strong&gt;Solution&lt;/strong&gt;: Run &lt;tt&gt;ollama pull \&lt;model_name\&gt;&lt;/tt&gt;

&lt;strong&gt;Issue&lt;/strong&gt;: "LLM returned empty command list"  &lt;br&gt;
&lt;strong&gt;Solution&lt;/strong&gt;: Rephrase prompt to be more specific
```

&lt;hr&gt;

@subsection autotoc_md1647 5. Implementation Timeline

@subsubsection autotoc_md1648 Week 1 (October 7-11)
- &lt;strong&gt;Day 1-2&lt;/strong&gt;: Implement &lt;tt&gt;OllamaAIService&lt;/tt&gt; class
- &lt;strong&gt;Day 3&lt;/strong&gt;: Wire into agent commands with service selection
- &lt;strong&gt;Day 4&lt;/strong&gt;: Testing and validation
- &lt;strong&gt;Day 5&lt;/strong&gt;: Documentation and examples

@subsubsection autotoc_md1649 Week 2 (October 14-18)
- &lt;strong&gt;Day 1&lt;/strong&gt;: Fix and improve &lt;tt&gt;GeminiAIService&lt;/tt&gt;
- &lt;strong&gt;Day 2&lt;/strong&gt;: Implement &lt;tt&gt;ClaudeAIService&lt;/tt&gt;
- &lt;strong&gt;Day 3&lt;/strong&gt;: Enhanced prompt engineering with resource catalogue
- &lt;strong&gt;Day 4&lt;/strong&gt;: Integration tests across all services
- &lt;strong&gt;Day 5&lt;/strong&gt;: User guide and troubleshooting docs

&lt;hr&gt;

@subsection autotoc_md1651 6. Success Criteria

✅ &lt;strong&gt;Phase 1 Complete When&lt;/strong&gt;:
- Ollama service connects and generates valid commands
- &lt;tt&gt;z3ed agent run&lt;/tt&gt; works end-to-end with local LLM
- Health checks report clear error messages
- Test script passes on macOS with Ollama installed

✅ &lt;strong&gt;Phase 2 Complete When&lt;/strong&gt;:
- Gemini API calls succeed with valid responses
- Markdown code block stripping works reliably
- Error messages are actionable (e.g., "API key invalid")

✅ &lt;strong&gt;Phase 3 Complete When&lt;/strong&gt;:
- Claude service implemented with same interface
- All three services (Ollama, Gemini, Claude) work interchangeably
- Service selection mechanism is transparent to user

✅ &lt;strong&gt;Phase 4 Complete When&lt;/strong&gt;:
- System prompts include full resource catalogue
- Few-shot examples improve command accuracy &gt;90%
- LLM responses consistently match expected command format

&lt;hr&gt;

@subsection autotoc_md1653 7. Future Enhancements (Post-MVP)

- &lt;strong&gt;Response Caching&lt;/strong&gt;: Cache LLM responses by prompt hash to reduce costs/latency
- &lt;strong&gt;Token Usage Tracking&lt;/strong&gt;: Monitor and report token consumption per session
- &lt;strong&gt;Model Comparison&lt;/strong&gt;: A/B test different models for accuracy/cost trade-offs
- &lt;strong&gt;Fine-Tuning&lt;/strong&gt;: Fine-tune local models on z3ed command corpus
- &lt;strong&gt;Multi-Turn Dialogue&lt;/strong&gt;: Support follow-up questions and clarifications
- &lt;strong&gt;Agentic Loop&lt;/strong&gt;: LLM self-corrects based on execution results
- &lt;strong&gt;GUI Integration&lt;/strong&gt;: In-app AI assistant panel in YAZE editor

&lt;hr&gt;

@subsection autotoc_md1655 Appendix A: Recommended Models

&lt;table class="markdownTable"&gt;
  &lt;tr class="markdownTableHead"&gt;    &lt;th class="markdownTableHeadNone"&gt; Model   </p>
<p>Provider   </p>
<p>Size   </p>
<p>Speed   </p>
<p>Accuracy   </p>
<p>Use Case    </p>
<p>qwen2.5-coder:7b   </p>
<p>Ollama   </p>
<p>7B   </p>
<p>Fast   </p>
<p>High   </p>
<p><b>Recommended</b>: Best balance    </p>
<p>codellama:13b   </p>
<p>Ollama   </p>
<p>13B   </p>
<p>Medium   </p>
<p>Higher   </p>
<p>Complex tasks    </p>
<p>llama3:70b   </p>
<p>Ollama   </p>
<p>70B   </p>
<p>Slow   </p>
<p>Highest   </p>
<p>Maximum accuracy    </p>
<p>gemini-2.5-flash   </p>
<p>Gemini   </p>
<p>N/A   </p>
<p>Fast   </p>
<p>High   </p>
<p>Remote option, low cost    </p>
<p>claude-3.5-sonnet   </p>
<p>Claude   </p>
<p>N/A   </p>
<p>Medium   </p>
<p>Highest   </p>
<p>Premium remote option   </p>
<h2><a class="anchor" id="autotoc_md1656"></a>
Appendix B: Example Prompts</h2>
<p><b>Simple</b>:</p><ul>
<li>"Validate the ROM"</li>
<li>"Export the first palette"</li>
<li>"Show ROM info"</li>
</ul>
<p><b>Moderate</b>:</p><ul>
<li>"Make soldier armor red"</li>
<li>"Change tile at (10, 20) in Light World to grass"</li>
<li>"Export dungeon room 5 to /tmp/room5.bin"</li>
</ul>
<p><b>Complex</b>:</p><ul>
<li>"Find all palettes using color #FF0000 and change to #00FF00"</li>
<li>"Export all dungeon rooms, modify object 3 in each, then reimport"</li>
<li>"Generate a comparison report between two ROM versions"</li>
</ul>
<hr  />
<h2><a class="anchor" id="autotoc_md1658"></a>
Next Steps</h2>
<p><b>👉 START HERE</b>: Implement Phase 1 (Ollama Integration) by following section 1.1-1.4 above.</p>
<p>Once complete, update this document with:</p><ul>
<li>Actual time spent vs. estimates</li>
<li>Issues encountered and solutions</li>
<li>Model performance observations</li>
<li>User feedback</li>
</ul>
<p><b>Questions? Blockers?</b> Open an issue or ping @scawful in Discord. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="../../doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8 </li>
  </ul>
</div>
</body>
</html>
